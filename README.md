# Distillation Demo: What Anthropic Accused DeepSeek of Doing

**For video: "I Replicated What Anthropic Accused DeepSeek of Doing"**

On February 24, 2026, Anthropic published a blog post accusing DeepSeek, MiniMax,
and Moonshot AI of running "industrial-scale distillation campaigns" against Claude,
using 24,000 fraudulent accounts and 16 million exchanges to extract coding and
agentic reasoning capabilities.

This demo recreates the same technique at toy scale (~100 examples instead of 16M)
to show what distillation actually is, how it works, and why the story is more
nuanced than the headlines suggest. We start with 32 hand-written coding prompts
and expand them to ~100 via programmatic variations (adding error handling, tests,
complexity analysis, etc.) — a miniature version of how real campaigns generate
prompt diversity at scale.

## Quick Start

```bash
# Install dependencies
pip install -r requirements.txt

# Step 1: Collect training data from Claude (~$1.50-2.00, takes ~5 min)
export ANTHROPIC_API_KEY=your-key-here
python 01_collect_teacher_data.py

# Step 2: Fine-tune a small model on Claude's outputs (~5-15 min on Apple Silicon/GPU)
python 02_finetune_student.py

# Step 3: Compare the models (the money shot)
python 03_compare_models.py
```

## Hardware Requirements

- **Step 1**: Just needs internet access and an Anthropic API key
- **Step 2**: Apple Silicon Macs (M1+) or a GPU with 8+ GB VRAM.
  An M4 Max finishes in ~5-10 minutes. CPU-only is possible but slow (~1-2 hours).
- **Step 3**: Same as Step 2 (runs inference on the local models)

## File Overview

| File                                                | Purpose                                       |
|-----------------------------------------------------|-----------------------------------------------|
| [`01_collect_teacher_data.py`](01_collect_teacher_data.py) | Query Claude, save responses as training data |
| [`02_finetune_student.py`](02_finetune_student.py)         | Fine-tune a small model on those responses    |
| [`03_compare_models.py`](03_compare_models.py)             | Side-by-side comparison (the live demo)       |
| [`requirements.txt`](requirements.txt)                     | Python dependencies                           |
| [`generate_visuals.py`](generate_visuals.py)               | Creates charts for presenting results         |
| [`RESULTS.md`](RESULTS.md)                                 | Sample results from a full test run           |
| [`how-it-works.md`](how-it-works.md)                       | Deep dive: API usage, LoRA, and training      |
| `teacher_data.jsonl`                                       | Generated by Step 1 (not committed)           |
| `distilled-student/`                                       | Generated by Step 2 (not committed)           |

## Customization

**Swap the student model** — Use `--model` in Steps 2 and 3. Step 3 reads the
base model from the saved adapter config automatically, so you only need to
specify it once during training:
```bash
python 02_finetune_student.py --model "Qwen/Qwen2.5-1.5B-Instruct"
python 03_compare_models.py   # auto-detects the base model
```

Available models (roughly by size):
- `Qwen/Qwen2.5-0.5B-Instruct` — fastest, fits anywhere
- `TinyLlama/TinyLlama-1.1B-Chat-v1.0` — slightly better results
- `Qwen/Qwen2.5-1.5B-Instruct` — good balance of quality and speed (default)
- `microsoft/Phi-3-mini-4k-instruct` — best results, needs ~10GB VRAM

**Add more prompts** — The 32 base prompts are expanded to ~100 via
programmatic modifiers (error handling, tests, complexity analysis, etc.).
You can add more base prompts to `CODING_PROMPTS` or add new modifiers to
`PROMPT_MODIFIERS` in `01_collect_teacher_data.py`. At 16 million,
you'd see why Anthropic is concerned.

**Target different capabilities** — We targeted coding, but you could target
creative writing, reasoning, tool use, or anything else. The Chinese labs
allegedly focused on "agentic reasoning, tool use, and coding."

## License

MIT. The irony of licensing a distillation demo is not lost on us.
